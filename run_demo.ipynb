{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409f39ea-9d44-4eec-b409-0360c47e00bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 14:03:11.986303: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-17 14:03:12.022183: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-17 14:03:12.022211: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-17 14:03:12.022232: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-17 14:03:12.028812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 14:03:12.890396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/chemllm/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/ylee/chemllm/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/ylee/chemllm/lib/python3.11/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# This means you will use the first GPU among the four GPUs in our case.\n",
    "# \"0\", \"1\", \"2\", \"3\". Since FT dataset is small, using one GPU should be proper.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import lightning as L\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Filter out FutureWarning and UnderReviewWarning messages from pl_bolts\n",
    "warnings.filterwarnings(\"ignore\", module=\"pl_bolts\")\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append( '../')\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import tokenizer_sol, datamodule_finetune_sol, model_finetune_sol, chemllama_mtr, utils_sol\n",
    "import auto_evaluator_sol\n",
    "\n",
    "# print(os.path.dirname(__file__))\n",
    "\n",
    "\n",
    "torch.manual_seed(1004)\n",
    "np.random.seed(1004)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6266b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Pretrained-model from git repo\n",
    "utils_sol.get_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bde15fc-8bf0-4b67-a19b-cd56056485d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyper Parameters ##### <- You can control these parameters as you want\n",
    "# solute_or_solvent = 'solvent'\n",
    "solute_or_solvent = 'solute'\n",
    "ver_ft = 0 # version control for FT model & evaluation data # Or it will overwrite the models and results\n",
    "batch_size_pair = [64, 64] if solute_or_solvent == 'solute' else [10, 10] # [train, valid(test)] \n",
    "# since 'solute' has very small dataset. So I thinl 10 for train and 10 for valid(test) should be the maximum values.\n",
    "lr = 0.0001 \n",
    "epochs = 7\n",
    "use_freeze = False  # Freeze the model or not # False measn not freezing\n",
    "overwrite_level_2 = True # If you don't want to overwrite the models and csv files, then change this to False\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3325877-0cc8-40a7-aa28-612a06e7fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "tokenizer = tokenizer_sol.fn_load_tokenizer_llama(\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "max_length = max_seq_length\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ecff8d-1e37-49f5-9f54-839a44d99ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "ep = 7 # this is the target epoch to load the trained fine-tuned model\n",
    "# dir_main = \"/home/ylee/SolLlama\"\n",
    "dir_main = \"./\"\n",
    "#################################\n",
    "name_model_mtr = \"ChemLlama_Medium_30m_vloss_val_loss=0.029_ep_epoch=04.ckpt\" \n",
    "\n",
    "dir_model_mtr = f\"{dir_main}/model_mtr/{name_model_mtr}\"\n",
    "\n",
    "ver_ft = 0\n",
    "dir_model_ft_to_save = f\"{dir_main}/save_models_ft/ft_version_{ver_ft}\"\n",
    "name_model_ft = f'AbraLlama_{solute_or_solvent}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286c8c2f-0ff9-4043-ae84-9e5e57b191b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/chemllm/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /opt/software/jupyter-server/2.7.2-GCCcore-12.3.0/l ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ylee/chemllm/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /opt/software/jupyter-server/2.7.2-GCCcore-12.3.0/l ...\n",
      "/home/ylee/chemllm/lib/python3.11/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory .//save_models_ft/ft_version_0/AbraLlama_solute/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/ylee/chemllm/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory .//save_models_ft/ft_version_0/AbraLlama_solute/version_0/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ylee/SolLlama/model_finetune_sol.py:131: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | model_mtr  | ChemLlama | 29.4 M\n",
      "1 | gelu       | GELU      | 0     \n",
      "2 | linear1    | Linear    | 6.8 K \n",
      "3 | linear2    | Linear    | 4.2 K \n",
      "4 | regression | Linear    | 325   \n",
      "5 | loss_fn    | L1Loss    | 0     \n",
      "-----------------------------------------\n",
      "29.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.5 M    Total params\n",
      "117.835   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fbf69ee732444b9473719607ba7ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=7` reached.\n",
      "/home/ylee/chemllm/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /opt/software/jupyter-server/2.7.2-GCCcore-12.3.0/l ...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee19e88ff384bc18a0579801f1f0034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ylee/SolLlama/datamodule_finetune_sol.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"labels\": torch.tensor(self.labels.iloc[idx]),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.07019355148077011    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.07019355148077011   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.07019355148077011}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset for finetune\n",
    "batch_size_for_train = batch_size_pair[0]\n",
    "batch_size_for_valid = batch_size_pair[1]\n",
    "\n",
    "data_module = datamodule_finetune_sol.CustomFinetuneDataModule(\n",
    "    solute_or_solvent=solute_or_solvent,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_length,\n",
    "    batch_size_train=batch_size_for_train,\n",
    "    batch_size_valid=batch_size_for_valid,\n",
    "    # num_device=int(config.NUM_DEVICE) * config.NUM_WORKERS_MULTIPLIER,\n",
    "    num_device=num_workers,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "steps_per_epoch = len(data_module.train_dataloader())\n",
    "\n",
    "# Load model and optimizer for finetune\n",
    "learning_rate = lr\n",
    "\n",
    "\n",
    "model_mtr = chemllama_mtr.ChemLlama.load_from_checkpoint(dir_model_mtr)\n",
    "\n",
    "\n",
    "model_ft = model_finetune_sol.CustomFinetuneModel(\n",
    "    model_mtr=model_mtr,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    warmup_epochs=1,\n",
    "    max_epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    # dataset_dict=dataset_dict,\n",
    "    use_freeze=use_freeze,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    filename=name_model_ft + '_{epoch:02d}',\n",
    "    every_n_epochs=1,\n",
    "    save_top_k=-1,\n",
    "    enable_version_counter=False, # keep the version == 0\n",
    "    save_weights_only=True,\n",
    ")\n",
    "checkpoint_callback.FILE_EXTENSION = \".pt\"\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir=dir_model_ft_to_save,\n",
    "    name=name_model_ft,\n",
    "    version=0,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=dir_model_ft_to_save,\n",
    "    # profiler=profiler,\n",
    "    logger=csv_logger,\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    # accelerator='gpu',\n",
    "    # devices=[0],\n",
    "    min_epochs=1,\n",
    "    max_epochs=epochs,\n",
    "    precision=32,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model_ft, data_module)\n",
    "trainer.validate(model_ft, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845864-7131-464a-adfd-fb8aa57c0e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d86c24-1019-4402-9c5a-0cc0a3ac2592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with epoch 7\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m local_model_ft \u001b[38;5;241m=\u001b[39m \u001b[43mutils_sol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_ft_with_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_model_ft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdir_model_ft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_model_ft_to_save\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_model_ft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_model_ft\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(local_model_ft, data_module)\n\u001b[1;32m      9\u001b[0m result_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/SolLlama/utils_sol.py:95\u001b[0m, in \u001b[0;36mload_model_ft_with_epoch\u001b[0;34m(class_model_ft, target_epoch, dir_model_ft, name_model_ft)\u001b[0m\n\u001b[1;32m     92\u001b[0m list_model_ft_in_the_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(list_files_in_dir_model_ft, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model with epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m dir_target_model_ft \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_all_model_ft\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlist_model_ft_in_the_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_epoch\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# class_model_ft.load_from_checkpoint(dir_target_model_ft)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m loaded_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(dir_target_model_ft)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "local_model_ft = utils_sol.load_model_ft_with_epoch(\n",
    "    class_model_ft=model_ft, \n",
    "    target_epoch=ep,\n",
    "    dir_model_ft=dir_model_ft_to_save,\n",
    "    name_model_ft=name_model_ft\n",
    ")\n",
    "\n",
    "result = trainer.predict(local_model_ft, data_module)\n",
    "result_pred = list()\n",
    "result_label = list()\n",
    "for bat in range(len(result)):\n",
    "    result_pred.append(result[bat][0].squeeze())\n",
    "    result_label.append(result[bat][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b591266-8143-473e-9e8d-57c80a5e2fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcc91f-526a-4adc-b427-de5e2009a5f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a3671-ff4c-404d-b914-24e5f205eb6d",
   "metadata": {},
   "source": [
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f58f5-8032-477f-ba7a-aa8f79fab92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.test_df['SMILES'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a251f-eb9c-482c-8ee8-be1e8b2c24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_predict = list()\n",
    "for i in result_pred:\n",
    "    local_df_predict = pd.DataFrame(np.array(i))\n",
    "    list_df_predict.append(local_df_predict)\n",
    "\n",
    "df_predict = pd.concat(list_df_predict)\n",
    "df_predict.insert(0, \"SMILES\", data_module.test_df['SMILES'].reset_index(drop=True))\n",
    "df_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ca54f-822f-4d14-b732-78f96abb5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = data_module.test_df.reset_index(drop=True)\n",
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656b42f-d067-4fde-a292-90f2c4548542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemllm",
   "language": "python",
   "name": "chemllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
